# -*- coding: utf-8 -*-
"""PredictiveModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GhxVnchSkP4K5BE-69h05pKTCOX0t-QE
"""

import pandas as pd

"""#Data Cleaning"""

df = pd.read_csv('zameen.csv')
print(df.head())

df.isnull().sum()

df['agency'].fillna('Unknown Agency', inplace=True)
df['agent'].fillna('Unknown Agent', inplace=True)
#agency and agents had null values so i filled in those missing values with sample field

df['Area Size'] = pd.to_numeric(df['Area Size'], errors='coerce')
df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
#ensuring of all float values

print(df['baths'].value_counts().sort_index())

df = df[(df['baths'] >= 0) & (df['baths'] <= 8)]
#Max is 11 baths so applying range from 0 - 11

print(df['area'].value_counts().sort_index())

df = df[df['Area Size'] > 0]
#0 marla or 0 kanal size are not possible

df['date_added'] = pd.to_datetime(df['date_added'], dayfirst=True, errors='coerce')
df['date_added'] = df['date_added'].dt.strftime('%Y-%m-%d')
#formatting the date in Y-M-D format

df['date_added'].isnull().sum()
df = df.dropna(subset=['date_added'])  # or fill with a default year

df['property_id'].duplicated().sum()
#Can't repeat as unique for every property as there is no duplicates so need to remove

df['location_id'].duplicated().sum()
#can repeat

"""#Data Exploration"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df['price'], bins=50, kde=True)
plt.title('Distribution of Property Prices')
plt.xlabel('Price')
plt.ylabel('Count')
plt.show()

sns.scatterplot(data=df, x='Area Size', y='price')
plt.title('Area Size vs Price')
plt.xlabel('Area Size')
plt.ylabel('Price')
plt.show()

city_price = df.groupby('city')['price'].mean().sort_values(ascending=False)

sns.barplot(x=city_price.index, y=city_price.values)
plt.xticks(rotation=45)
plt.title('Average Price by City')
plt.ylabel('Average Price')
plt.show()

sns.boxplot(data=df, x='property_type', y='price')
plt.xticks(rotation=45)
plt.title('Property Type vs Price')
plt.show()

sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""#Feature Engineering"""

from datetime import datetime
import numpy as np
df['listing_year'] = pd.to_datetime(df['date_added'], errors='coerce').dt.year
df['property_age'] = datetime.now().year - df['listing_year']
#age of property
df['bed_per_bath'] = df['bedrooms'] / df['baths']
df['bed_per_bath'].replace([np.inf, -np.inf], np.nan, inplace=True)
#bed per bath
df['price_per_unit'] = df['price'] / df['Area Size']
#price per marla or kanal

"""#Encoding"""

print(df.head())

df.drop(['property_id', 'agent', 'agency', 'page_url'], axis=1, inplace=True)
#dropping unneccessary cols

df = pd.get_dummies(df, columns=['property_type', 'purpose', 'province_name'], drop_first=True)
#one hot encoding as they are not unique mostly similar and will help determine us where the bulk of propertie are bought in

import pickle
from sklearn.preprocessing import LabelEncoder

categorical_cols = ['city', 'area', 'date_added', 'Area Type','location','Area Category']
encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    encoders[col] = le

# Save the encoders
with open('encoders.pkl', 'wb') as f:
    pickle.dump(encoders, f)

"""#Outlier Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))
sns.boxplot(x=df['price'])
plt.title("Price Distribution (Boxplot)")
plt.show()

plt.figure(figsize=(10, 5))
sns.histplot(df['price'], bins=50, kde=True)
plt.title("Price Distribution (Histogram)")
plt.show()

Q1 = df['price'].quantile(0.25)
Q3 = df['price'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['price'] < lower_bound) | (df['price'] > upper_bound)]
print(f"Total outliers detected: {len(outliers)}")

# Compare outliers with non-outliers
compare = df.copy()
compare['is_outlier'] = (df['price'] < lower_bound) | (df['price'] > upper_bound)

# See average values by outlier status
print(compare.groupby('is_outlier')[['bedrooms', 'baths', 'Area Size']].mean())

# Common locations for outliers
print(compare[compare['is_outlier']]['location'].value_counts().head())

# See a few example rows
print(outliers[['price', 'bedrooms', 'baths', 'Area Size', 'location', 'Area Type']].head())

"""#Predictive Modeling"""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Train-test split
from sklearn.model_selection import train_test_split

# Drop non-numeric or unused columns (you can refine this based on your EDA)
features_to_drop = ['price', 'page_url', 'agency', 'agent', 'date_added']
X = df.drop(columns=features_to_drop, errors='ignore')
y = df['price']

# Handle categorical variables (simple label encoding or one-hot if needed)
X = pd.get_dummies(X)

# Save feature columns to use during prediction
feature_columns = X.columns

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
lr = LinearRegression()
rf = RandomForestRegressor(random_state=42)
gb = GradientBoostingRegressor(random_state=42)

lr.fit(X_train, y_train)
rf.fit(X_train, y_train)
gb.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score

models = {'Linear Regression': lr, 'Random Forest': rf, 'Gradient Boosting': gb}

for name, model in models.items():
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"{name} - MSE: {mse:.2f}, R²: {r2:.2f}")

import pickle

# Save one model (e.g., gradient boosting) and columns
with open("model.pkl", "wb") as f:
    pickle.dump(gb, f)

with open("features.pkl", "wb") as f:
    pickle.dump(feature_columns, f)

"""#Predicting Model"""

X = pd.get_dummies(X)
feature_columns = X.columns

import pickle
with open('features.pkl', 'wb') as f:
    pickle.dump(feature_columns, f)

import pandas as pd
import pickle

# Load model and feature columns
with open("model.pkl", "rb") as f:
    model = pickle.load(f)
with open("features.pkl", "rb") as f:
    feature_columns = pickle.load(f)

# Example new input (use same columns as model expects)
new_data = {
    'location_id': 3325,
    'latitude': 33.679890,
    'longitude': 73.012640,
    'baths': 2,
    'bedrooms': 3,
    'Area Size': 5.0,
    'city_Islamabad': 1,
    'province_name_Islamabad Capital': 1,
    'property_type_House': 1,
    'purpose_For Sale': 1,
    # all other possible one-hot columns must be 0
}

# Create DataFrame
new_df = pd.DataFrame([new_data])

# Add missing columns
for col in feature_columns:
    if col not in new_df.columns:
        new_df[col] = 0

# Reorder to match model input
new_df = new_df[feature_columns]

# Predict
prediction = model.predict(new_df)
print("Predicted Price:", prediction[0])

"""#Model Performance"""

from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))
print("R² Score:", r2_score(y_test, y_pred))

import matplotlib.pyplot as plt

importances = model.feature_importances_
features = feature_columns

plt.figure(figsize=(10, 6))
plt.barh(features, importances)
plt.xlabel("Feature Importance")
plt.title("What Features Influence Price")
plt.tight_layout()
plt.show()

import pickle

with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

with open("features.pkl", "wb") as f:
    pickle.dump(feature_columns, f)